from pathlib import Path
import hashlib
import json
import base64
import asyncio
import os
import time
import traceback
from utils.platform_config import IS_MACOS, IS_WINDOWS
from utils.message_format import ClipMessage, MessageType
from config import ClipboardConfig

# Only import AppKit and objc on macOS
if IS_MACOS:
    import AppKit
    import objc # Import objc

    # Helper class to perform pasteboard operations on the main thread
    class PasteboardSetter(AppKit.NSObject):
        @classmethod # Use standard Python classmethod decorator
        def setFileURL_(cls, path_str):
            try:
                pasteboard = AppKit.NSPasteboard.generalPasteboard()
                pasteboard.clearContents()
                url = AppKit.NSURL.fileURLWithPath_(path_str)
                if not url:
                    print(f"âŒ [MainThread] æ— æ³•åˆ›å»ºæ–‡ä»¶URL: {path_str}")
                    return "0|-1"
                urls = AppKit.NSArray.arrayWithObject_(url)
                success = pasteboard.writeObjects_(urls)
                if success:
                    change_count = pasteboard.changeCount()
                    print(f"ğŸ“ [MainThread] å·²å°†æ–‡ä»¶æ·»åŠ åˆ°Macå‰ªè´´æ¿: {Path(path_str).name}")
                    return f"1|{change_count}"
                else:
                    print(f"âŒ [MainThread] æ·»åŠ æ–‡ä»¶åˆ°Macå‰ªè´´æ¿å¤±è´¥: {Path(path_str).name}")
                    return "0|-1"
            except Exception as e:
                print(f"âŒ [MainThread] è®¾ç½®å‰ªè´´æ¿æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                import traceback
                traceback.print_exc()
                return "0|-1"


class FileHandler:
    """æ–‡ä»¶å¤„ç†ç®¡ç†å™¨"""
    
    def __init__(self, temp_dir: Path, security_mgr):
        self.temp_dir = temp_dir
        self.security_mgr = security_mgr
        self.file_transfers = {}
        self.file_cache = {}
        self._init_temp_dir()
        self.load_file_cache()
        self.chunk_size = ClipboardConfig.CHUNK_SIZE # Use config
        self.pending_transfers = {}  # Track ongoing chunked transfers

    def _init_temp_dir(self):
        """åˆå§‹åŒ–ä¸´æ—¶ç›®å½•"""
        self.temp_dir.mkdir(exist_ok=True)
        print(f"âœ… æ–‡ä»¶å¤„ç†åˆå§‹åŒ–æˆåŠŸï¼Œä¸´æ—¶ç›®å½•: {self.temp_dir}")

    def _looks_like_temp_file_path(self, text: str) -> bool:
        """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦çœ‹èµ·æ¥åƒä¸´æ—¶æ–‡ä»¶è·¯å¾„"""
        for indicator in ClipboardConfig.TEMP_PATH_INDICATORS:
            if indicator in text:
                print(f"â­ï¸ è·³è¿‡ä¸´æ—¶æ–‡ä»¶è·¯å¾„: \"{text[:40]}...\"")
                return True
        return False

    async def handle_file_transfer(self, file_path: str, send_encrypted_fn):
        """å¤„ç†æ–‡ä»¶ä¼ è¾“ï¼ˆè‡ªåŠ¨åˆ†å—å¤§æ–‡ä»¶ï¼‰"""
        path_obj = Path(file_path)
        MAX_CHUNK_SIZE = self.chunk_size # Use instance chunk size

        if not path_obj.exists() or not path_obj.is_file():
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨æˆ–æ— æ•ˆ: {file_path}")
            return False

        try:
            file_size = path_obj.stat().st_size
            total_chunks = (file_size + MAX_CHUNK_SIZE - 1) // MAX_CHUNK_SIZE
            print(f"ğŸ“¤ å¼€å§‹ä¼ è¾“æ–‡ä»¶: {path_obj.name} ({file_size/1024/1024:.1f}MB, {total_chunks}å—)")

            # å‘é€æ–‡ä»¶å¼€å§‹æ¶ˆæ¯ (optional, could be part of the first chunk)
            # Consider if a separate start message is needed or if info can be in first chunk

            # é€å—è¯»å–å¹¶å‘é€æ–‡ä»¶
            with open(path_obj, 'rb') as f:
                for chunk_index in range(total_chunks):
                    chunk_data = f.read(MAX_CHUNK_SIZE)
                    if not chunk_data:
                        break

                    chunk_msg = {
                        'type': MessageType.FILE_RESPONSE,
                        'filename': path_obj.name,
                        'exists': True,
                        'chunk_data': base64.b64encode(chunk_data).decode('utf-8'),
                        'chunk_index': chunk_index,
                        'total_chunks': total_chunks,
                        'chunk_hash': hashlib.md5(chunk_data).hexdigest(),
                        'file_hash': ClipMessage.calculate_file_hash(str(path_obj)) if chunk_index == 0 else None # Send full hash only once
                    }

                    # æ˜¾ç¤ºè¿›åº¦
                    progress = self._format_progress(chunk_index + 1, total_chunks)
                    print(f"\rğŸ“¤ ä¼ è¾“æ–‡ä»¶ {path_obj.name}: {progress}", end="", flush=True)

                    # åŠ å¯†å¹¶å‘é€å—
                    await send_encrypted_fn(json.dumps(chunk_msg).encode('utf-8'))
                    await asyncio.sleep(ClipboardConfig.NETWORK_DELAY) # Use config

            print(f"\nâœ… æ–‡ä»¶ {path_obj.name} ä¼ è¾“å®Œæˆ")
            return True

        except Exception as e:
            print(f"\nâŒ æ–‡ä»¶ä¼ è¾“å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False

    # Removed _transfer_small_file as handle_file_transfer now handles chunking

    # Removed send_large_file and _send_file_chunk as handle_file_transfer covers this

    # Removed receive_file_chunk as handle_received_chunk covers this

    # Removed _complete_file_transfer as handle_received_chunk handles completion

    def _format_progress(self, current: int, total: int) -> str:
        """æ ¼å¼åŒ–è¿›åº¦æ˜¾ç¤º"""
        if total <= 0: return "[â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘] 0% (0/0)" # Avoid division by zero
        percentage = (current * 100) // total
        bar_length = 20
        filled = min(bar_length, (percentage * bar_length) // 100) # Ensure filled doesn't exceed bar_length
        bar = 'â–ˆ' * filled + 'â–‘' * (bar_length - filled)
        return f"[{bar}] {percentage}% ({current}/{total})"

    def handle_received_chunk(self, message: dict) -> tuple[bool, Path | None]:
        """
        å¤„ç†æ¥æ”¶åˆ°çš„æ–‡ä»¶å—.
        Returns: (is_complete, file_path_if_complete)
        """
        try:
            filename = message.get("filename", "unknown")
            chunk_index = message.get("chunk_index", 0)
            total_chunks = message.get("total_chunks", 1)
            chunk_data = base64.b64decode(message.get("chunk_data", ""))
            chunk_hash = message.get("chunk_hash")
            file_hash = message.get("file_hash") # Full file hash (sent with first chunk)

            if not chunk_data:
                print("âš ï¸ æ”¶åˆ°çš„æ–‡ä»¶å—æ•°æ®ä¸ºç©º")
                return False, None

            # éªŒè¯å—çš„å®Œæ•´æ€§
            if chunk_hash and hashlib.md5(chunk_data).hexdigest() != chunk_hash:
                print(f"âš ï¸ å— {chunk_index+1}/{total_chunks} æ ¡éªŒå¤±è´¥ for {filename}")
                # Optionally request retransmission here
                return False, None

            save_path = self.temp_dir / filename

            # Initialize transfer state if first chunk
            if filename not in self.file_transfers:
                self.file_transfers[filename] = {
                    "received_chunks": {}, # Store data by index
                    "total_chunks": total_chunks,
                    "path": save_path,
                    "file_hash": file_hash # Store the expected full hash
                }
                # Clear any old file with the same name
                if save_path.exists():
                    try:
                        save_path.unlink()
                    except OSError as e:
                        print(f"âš ï¸ æ— æ³•åˆ é™¤æ—§æ–‡ä»¶ {save_path}: {e}")


            transfer = self.file_transfers[filename]

            # Store chunk data if not already received
            if chunk_index not in transfer["received_chunks"]:
                 transfer["received_chunks"][chunk_index] = chunk_data
            else:
                 print(f"â„¹ï¸ æ”¶åˆ°é‡å¤å— {chunk_index+1}/{total_chunks} for {filename}")


            # Display progress
            progress = self._format_progress(len(transfer["received_chunks"]), transfer["total_chunks"])
            print(f"\rğŸ“¥ æ¥æ”¶æ–‡ä»¶ {filename}: {progress}", end="", flush=True)


            # æ£€æŸ¥æ˜¯å¦å®Œæˆ
            is_complete = len(transfer["received_chunks"]) == transfer["total_chunks"]

            if is_complete:
                print(f"\nâœ… æ–‡ä»¶ {filename} æ‰€æœ‰å—æ¥æ”¶å®Œæˆï¼Œå¼€å§‹ç»„è£…...")
                # ç»„è£…æ–‡ä»¶
                try:
                    with open(save_path, "wb") as f:
                        for i in range(transfer["total_chunks"]):
                            if i in transfer["received_chunks"]:
                                f.write(transfer["received_chunks"][i])
                            else:
                                # This shouldn't happen if is_complete is true, but as a safeguard
                                print(f"âŒ ç»„è£…æ–‡ä»¶ {filename} æ—¶ç¼ºå°‘å— {i+1}")
                                raise IOError(f"Missing chunk {i+1} for {filename}")

                    # éªŒè¯å®Œæ•´æ–‡ä»¶å“ˆå¸Œ
                    if transfer["file_hash"]:
                        actual_hash = ClipMessage.calculate_file_hash(str(save_path))
                        if actual_hash == transfer["file_hash"]:
                            print(f"âœ… æ–‡ä»¶ {filename} å“ˆå¸Œæ ¡éªŒæˆåŠŸ")
                        else:
                            print(f"âŒ æ–‡ä»¶ {filename} å“ˆå¸Œæ ¡éªŒå¤±è´¥! Expected: {transfer['file_hash']}, Got: {actual_hash}")
                            # Optionally delete the corrupted file
                            # save_path.unlink()
                            del self.file_transfers[filename]
                            return False, None # Indicate failure
                    else:
                         print(f"âš ï¸ æœªæ”¶åˆ°æ–‡ä»¶ {filename} çš„å®Œæ•´å“ˆå¸Œå€¼ï¼Œè·³è¿‡æ ¡éªŒ")


                    # Add to cache (using the verified hash if available)
                    final_hash = transfer["file_hash"] or ClipMessage.calculate_file_hash(str(save_path))
                    self.add_to_file_cache(final_hash, str(save_path))

                    # æ¸…ç†ä¼ è¾“çŠ¶æ€
                    completed_path = transfer["path"]
                    del self.file_transfers[filename]
                    return True, completed_path # Indicate completion and return path

                except Exception as e:
                    print(f"âŒ ç»„è£…æˆ–æ ¡éªŒæ–‡ä»¶ {filename} å¤±è´¥: {e}")
                    # Clean up
                    if save_path.exists(): save_path.unlink(missing_ok=True)
                    if filename in self.file_transfers: del self.file_transfers[filename]
                    return False, None # Indicate failure

            return False, None # Indicate not yet complete

        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False, None # Indicate failure

    # Removed _verify_file_integrity as validation is now part of handle_received_chunk

    # --- File Cache Methods ---
    def load_file_cache(self):
        """åŠ è½½æ–‡ä»¶ç¼“å­˜"""
        cache_path = self.temp_dir / "filecache.json"
        try:
            if cache_path.exists():
                with open(cache_path, "r") as f:
                    self.file_cache = json.load(f)
                print(f"ğŸ“š å·²åŠ è½½ {len(self.file_cache)} ä¸ªæ–‡ä»¶ç¼“å­˜æ¡ç›®")
            else:
                self.file_cache = {}
                print("ğŸ“ åˆ›å»ºæ–°çš„æ–‡ä»¶ç¼“å­˜")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ–‡ä»¶ç¼“å­˜å¤±è´¥: {e}")
            self.file_cache = {}

    def save_file_cache(self):
        """ä¿å­˜æ–‡ä»¶ç¼“å­˜ä¿¡æ¯"""
        cache_path = self.temp_dir / "filecache.json"
        try:
            with open(cache_path, "w") as f:
                json.dump(self.file_cache, f)
        except Exception as e: # Catch specific exceptions if needed
            print(f"âŒ ä¿å­˜æ–‡ä»¶ç¼“å­˜å¤±è´¥: {e}")

    async def handle_text_message(self, message: dict, set_clipboard_func, 
                                 last_content_hash: str) -> tuple[str, float]:
        """å¤„ç†æ¥æ”¶åˆ°çš„æ–‡æœ¬æ¶ˆæ¯"""
        try:
            text = message.get("content", "")
            if not text:
                print("âš ï¸ æ”¶åˆ°ç©ºæ–‡æœ¬æ¶ˆæ¯")
                return last_content_hash, 0
            
            if self._looks_like_temp_file_path(text):
                return last_content_hash, 0
            
            # Calculate hash before setting clipboard
            from utils.clipboard_utils import ClipboardUtils
            content_hash = ClipboardUtils.calculate_content_hash(text)
            
            # Check if duplicate
            if content_hash == last_content_hash:
                print("â­ï¸ è·³è¿‡é‡å¤å†…å®¹")
                return last_content_hash, 0
            
            # Set clipboard using provided function
            if await set_clipboard_func(text):
                display_text = ClipboardUtils.format_display_content(text)
                print(f"ğŸ“¥ å·²å¤åˆ¶æ–‡æœ¬: \"{display_text}\"")
                return content_hash, time.time()
            else:
                print("âŒ æ›´æ–°å‰ªè´´æ¿å¤±è´¥")
                return last_content_hash, 0
                
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡æœ¬æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            traceback.print_exc()
            return last_content_hash, 0

    async def process_clipboard_content(self, text: str, current_time: float, 
                                      last_content_hash: str, last_update_time: float,
                                      broadcast_fn) -> tuple[str, float, bool]:
        """
        å¤„ç†å‰ªè´´æ¿æ–‡æœ¬å†…å®¹å¹¶å†³å®šæ˜¯å¦å‘é€
        Returns: (new_hash, new_time, update_sent)
        """
        try:
            if self._looks_like_temp_file_path(text):
                return last_content_hash, last_update_time, False
            
            # Calculate content hash
            content_hash = hashlib.md5(text.encode()).hexdigest()
            
            # Check if content has changed
            if content_hash == last_content_hash:
                return last_content_hash, last_update_time, False
            
            # Check minimum time interval between updates
            if current_time - last_update_time < ClipboardConfig.MIN_PROCESS_INTERVAL:
                return last_content_hash, last_update_time, False
            
            # Create and send text message
            message = {
                'type': MessageType.TEXT,
                'content': text,
                'timestamp': current_time
            }
            
            # Broadcast to all clients (broadcast_fn will check if clients exist)
            message_data = json.dumps(message).encode('utf-8')
            await broadcast_fn(message_data)
            
            # Display sent text (truncated)
            display_text = text[:ClipboardConfig.MAX_DISPLAY_LENGTH] + ("..." if len(text) > ClipboardConfig.MAX_DISPLAY_LENGTH else "")
            print(f"ğŸ“¤ å·²å‘é€æ–‡æœ¬: \"{display_text}\"")
            
            return content_hash, current_time, True
            
        except Exception as e:
            print(f"âŒ å¤„ç†å‰ªè´´æ¿æ–‡æœ¬å†…å®¹æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return last_content_hash, last_update_time, False

    async def handle_clipboard_files(self, file_paths: list, last_content_hash: str,
                                   broadcast_fn) -> tuple[str, bool]:
        """
        å¤„ç†å‰ªè´´æ¿æ–‡ä»¶å¹¶å‘é€æ–‡ä»¶ä¿¡æ¯
        Returns: (new_hash, update_sent)
        """
        try:
            if not file_paths:
                return last_content_hash, False
            
            # Calculate combined hash for all files
            files_hash = self.get_files_content_hash(file_paths)
            if not files_hash:
                return last_content_hash, False
            
            # Check if files have changed
            if files_hash == last_content_hash:
                return last_content_hash, False
            
            # Create file info message
            file_info_list = []
            for file_path in file_paths:
                path_obj = Path(file_path)
                if path_obj.exists() and path_obj.is_file():
                    file_info = {
                        'filename': path_obj.name,
                        'size': path_obj.stat().st_size,
                        'path': str(path_obj),
                        'hash': ClipMessage.calculate_file_hash(str(path_obj))
                    }
                    file_info_list.append(file_info)
            
            if not file_info_list:
                return last_content_hash, False
            
            # Send file info message (broadcast_fn will check if clients exist)
            message = {
                'type': MessageType.FILE,
                'files': file_info_list,
                'timestamp': time.time()
            }
            
            message_data = json.dumps(message).encode('utf-8')
            await broadcast_fn(message_data)
            
            # Display sent files
            file_names = [info['filename'] for info in file_info_list]
            print(f"ğŸ“¤ å·²å‘é€æ–‡ä»¶ä¿¡æ¯: {', '.join(file_names)}")
            
            return files_hash, True
            
        except Exception as e:
            print(f"âŒ å¤„ç†å‰ªè´´æ¿æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return last_content_hash, False

    def get_files_content_hash(self, file_paths: list) -> str:
        """è®¡ç®—å¤šä¸ªæ–‡ä»¶çš„ç»„åˆå“ˆå¸Œå€¼"""
        try:
            if not file_paths:
                return ""
            
            hasher = hashlib.md5()
            for file_path in sorted(file_paths):  # Sort for consistent hash
                path_obj = Path(file_path)
                if path_obj.exists() and path_obj.is_file():
                    # Add file path and modification time to hash
                    hasher.update(str(path_obj).encode())
                    hasher.update(str(path_obj.stat().st_mtime).encode())
                    # Could also add file size for more uniqueness
                    hasher.update(str(path_obj.stat().st_size).encode())
            
            return hasher.hexdigest()
            
        except Exception as e:
            print(f"âŒ è®¡ç®—æ–‡ä»¶å“ˆå¸Œæ—¶å‡ºé”™: {e}")
            return ""

    def add_to_file_cache(self, file_hash: str, file_path: str):
        """æ·»åŠ æ–‡ä»¶åˆ°ç¼“å­˜"""
        try:
            self.file_cache[file_hash] = {
                'path': file_path,
                'timestamp': time.time(),
                'filename': Path(file_path).name
            }
            self.save_file_cache()
        except Exception as e:
            print(f"âŒ æ·»åŠ æ–‡ä»¶ç¼“å­˜å¤±è´¥: {e}")

    async def handle_received_files(self, message: dict, send_encrypted_fn, sender_websocket=None):
        """å¤„ç†æ¥æ”¶åˆ°çš„æ–‡ä»¶ä¿¡æ¯ï¼Œè¯·æ±‚ç¼ºå¤±çš„æ–‡ä»¶"""
        try:
            files = message.get('files', [])
            if not files:
                print("âš ï¸ æ”¶åˆ°ç©ºçš„æ–‡ä»¶åˆ—è¡¨")
                return
            
            for file_info in files:
                filename = file_info.get('filename', 'unknown')
                file_hash = file_info.get('hash', '')
                file_size = file_info.get('size', 0)
                
                print(f"ğŸ“„ æ”¶åˆ°æ–‡ä»¶ä¿¡æ¯: {filename} ({file_size/1024/1024:.1f}MB)")
                
                # Check if we already have this file
                if file_hash in self.file_cache:
                    cached_path = Path(self.file_cache[file_hash]['path'])
                    if cached_path.exists():
                        print(f"âœ… æ–‡ä»¶ {filename} å·²å­˜åœ¨ç¼“å­˜ä¸­ï¼Œè·³è¿‡ä¸‹è½½")
                        continue
                
                # Request the file
                request_message = {
                    'type': MessageType.FILE_REQUEST,
                    'filename': filename,
                    'hash': file_hash,
                    'path': file_info.get('path', '')
                }
                
                request_data = json.dumps(request_message).encode('utf-8')
                await send_encrypted_fn(request_data)
                print(f"ğŸ“¨ å·²è¯·æ±‚æ–‡ä»¶: {filename}")
                
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()

    def set_clipboard_file(self, file_path: Path) -> int | None:
        """è®¾ç½®æ–‡ä»¶åˆ°å‰ªè´´æ¿ï¼Œè¿”å›å˜æ›´è®¡æ•°æˆ–Noneè¡¨ç¤ºå¤±è´¥"""
        if not IS_MACOS:
            print("âš ï¸ émacOSç³»ç»Ÿï¼Œæ— æ³•è®¾ç½®æ–‡ä»¶åˆ°å‰ªè´´æ¿")
            return None
        
        try:
            # Use AppKit from main thread via performSelectorOnMainThread
            path_str = str(file_path.resolve())
            
            # Use the PasteboardSetter class to set clipboard on main thread
            result = AppKit.NSThread.isMainThread()
            if result:  # Already on main thread
                result_str = PasteboardSetter.setFileURL_(path_str)
            else:  # Need to dispatch to main thread
                # Use performSelectorOnMainThread to execute on main thread
                result_str = objc.callmethod(
                    PasteboardSetter, 
                    "performSelectorOnMainThread:withObject:waitUntilDone:",
                    "setFileURL:",
                    path_str,
                    True  # Wait until done
                )
            
            # Parse result
            if isinstance(result_str, str) and '|' in result_str:
                success, change_count = result_str.split('|', 1)
                if success == '1':
                    return int(change_count)
            
            return None
            
        except Exception as e:
            print(f"âŒ è®¾ç½®å‰ªè´´æ¿æ–‡ä»¶å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None